{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 500)               50500     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 24)                12024     \n",
      "=================================================================\n",
      "Total params: 563,824\n",
      "Trainable params: 563,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'int'>\"}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-110b9f08dfd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m           verbose=True)\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    495\u001b[0m                      'at same time.')\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m   \u001b[1;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 653\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    654\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'int'>\"}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import HoverTool\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# read the data from excel home \n",
    "#data_mda_x = pd.read_excel('C:/Users/ERIC/Desktop/Jupyter/PML_AÑO_HORA_DIA.xlsx',2,)\n",
    "#data_mda_y = pd.read_excel('C:/Users/ERIC/Desktop/Jupyter/PML_AÑO_HORA_DIA.xlsx',3)\n",
    "\n",
    "# read the data from excel ORCA\n",
    "data_mda = pd.read_excel('C:/Users/Operaciones F/Forecast_Demand/PML 09LBR LUNES HORORIO.xlsx',2)\n",
    "#data_mda_y = pd.read_excel('C:/Users/Operaciones F/Forecast_Demand/PML 09LBR LUNES HORORIO.xlsx',3)\n",
    "\n",
    "'''\n",
    "# create a new plot with a title and axis labels\n",
    "p = figure(title=\"PML's DAILY BEHAVIOUR 2018\", x_axis_label='TIEMPO [DÍAS]', y_axis_label='[$ / MWh ]')\n",
    "\n",
    "# add the data\n",
    "\n",
    "p.line(data_mda['Num'], data_mda[1], legend_label = \"HORA 1\", color = 'blue')\n",
    "\n",
    "# show the results\n",
    "show(p)\n",
    "\n",
    "# output to static HTML file\n",
    "output_file(\"PML2018.html\")\n",
    "'''\n",
    "# Adding data to the training variables \n",
    "'''\n",
    "x_train = [\n",
    "           [data_mda[1]],[data_mda[2]],[data_mda[3]],[data_mda[4]],[data_mda[5]],[data_mda[6]],[data_mda[7]],\n",
    "           [data_mda[8]],[data_mda[9]],[data_mda[10]],[data_mda[11]],[data_mda[12]],[data_mda[13]],\n",
    "           [data_mda[14]],[data_mda[15]],[data_mda[16]],[data_mda[17]],[data_mda[18]],[data_mda[19]],\n",
    "           [data_mda[20]],[data_mda[21]],[data_mda[22]],[data_mda[23]],[data_mda[24]]\n",
    "            ]\n",
    "            \n",
    "y_train = [\n",
    "           [data_mtr[1]],[data_mtr[2]],[data_mtr[3]],[data_mtr[4]],[data_mtr[5]],[data_mtr[6]],[data_mtr[7]],\n",
    "           [data_mtr[8]],[data_mtr[9]],[data_mtr[10]],[data_mtr[11]],[data_mtr[12]],[data_mtr[13]],\n",
    "           [data_mtr[14]],[data_mtr[15]],[data_mtr[16]],[data_mtr[17]],[data_mtr[18]],[data_mtr[19]],\n",
    "           [data_mtr[20]],[data_mtr[21]],[data_mtr[22]],[data_mtr[23]],[data_mtr[24]]\n",
    "            ]\n",
    "         \n",
    "'''\n",
    "# Splitting the training and test set into a 80/20 proportion\n",
    "data_mda = data_mda.reindex(np.random.permutation(data_mda.index))\n",
    "mask  = np.random.rand(len(data_mda)) < 0.8\n",
    "data_mda_train = pd.DataFrame(data_mda[mask])\n",
    "data_mda_test = pd.DataFrame(data_mda[~mask])\n",
    "\n",
    "# Declaring the dataset for x tain\n",
    "x_train_1 = 1\n",
    "x_train_2 = 1\n",
    "\n",
    "# Declaring the dataset for y tain\n",
    "y_train = data_mda_train.iloc[:, 1:25].values\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Add\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Creating input layers\n",
    "model.add(Dense(100, input_shape = (2,)))\n",
    "\n",
    "# Creating hidden layers\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(500, activation = 'sigmoid'))\n",
    "\n",
    "# Creating the ouput\n",
    "model.add(Dense(24))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "# Now fit the model\n",
    "\n",
    "model.fit([x_train_1, x_train_2],\n",
    "          y_train,\n",
    "          epochs=10,\n",
    "          batch_size=1,\n",
    "          validation_split=0.2,\n",
    "          verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
