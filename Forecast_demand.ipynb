{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'pandas.core.series.Series\\'>\"})'}), <class 'NoneType'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6 arrays: [array([[1202.25811321, 1037.86056604,  933.06490566,  848.86433962,\n         819.67226415,  882.50150943, 1137.15641509, 1390.20679245,\n        1545.07981132, 1760.68113208, 1788.53566038, 1852.39622...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-477909892e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m           verbose=True)\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2517\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2518\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2519\u001b[1;33m           exception_prefix='target')\n\u001b[0m\u001b[0;32m   2520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2521\u001b[0m       \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    529\u001b[0m                        \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                        \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    532\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6 arrays: [array([[1202.25811321, 1037.86056604,  933.06490566,  848.86433962,\n         819.67226415,  882.50150943, 1137.15641509, 1390.20679245,\n        1545.07981132, 1760.68113208, 1788.53566038, 1852.39622..."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import HoverTool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the data from excel home \n",
    "data_mda = pd.read_excel('C:/Users/ERIC/Desktop/Jupyter/PML_AÑO_HORA_DIA.xlsx',2,)\n",
    "data_mtr = pd.read_excel('C:/Users/ERIC/Desktop/Jupyter/PML_AÑO_HORA_DIA.xlsx',3)\n",
    "\n",
    "# read the data from excel ORCA\n",
    "#data = pd.read_excel('C:/Users/Operaciones F/Forecast_Demand/Forecaste_PML_Daily_b.xlsx')\n",
    "'''\n",
    "\n",
    "# create a new plot with a title and axis labels\n",
    "p = figure(title=\"PML's DAILY BEHAVIOUR 2018\", x_axis_label='TIEMPO [MESES]', y_axis_label='[$ / MWh ]')\n",
    "\n",
    "# add the data\n",
    "\n",
    "p.line(data['MES'], data['lun_18'], legend_label = \"Lunes 2018\", color = 'blue')\n",
    "\n",
    "# show the results\n",
    "show(p)\n",
    "\n",
    "# output to static HTML file\n",
    "output_file(\"PML2018.html\")\n",
    "'''\n",
    "# Adding data to the training variables \n",
    "'''\n",
    "x_train = [\n",
    "           [data_mda[1]],[data_mda[2]],[data_mda[3]],[data_mda[4]],[data_mda[5]],[data_mda[6]],[data_mda[7]],\n",
    "           [data_mda[8]],[data_mda[9]],[data_mda[10]],[data_mda[11]],[data_mda[12]],[data_mda[13]],\n",
    "           [data_mda[14]],[data_mda[15]],[data_mda[16]],[data_mda[17]],[data_mda[18]],[data_mda[19]],\n",
    "           [data_mda[20]],[data_mda[21]],[data_mda[22]],[data_mda[23]],[data_mda[24]]\n",
    "            ]\n",
    "            \n",
    "y_train = [\n",
    "           [data_mtr[1]],[data_mtr[2]],[data_mtr[3]],[data_mtr[4]],[data_mtr[5]],[data_mtr[6]],[data_mtr[7]],\n",
    "           [data_mtr[8]],[data_mtr[9]],[data_mtr[10]],[data_mtr[11]],[data_mtr[12]],[data_mtr[13]],\n",
    "           [data_mtr[14]],[data_mtr[15]],[data_mtr[16]],[data_mtr[17]],[data_mtr[18]],[data_mtr[19]],\n",
    "           [data_mtr[20]],[data_mtr[21]],[data_mtr[22]],[data_mtr[23]],[data_mtr[24]]\n",
    "            ]\n",
    "'''          \n",
    "    \n",
    "from tensorflow.keras.layers import Dense, Add\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Dense(50, input_shape = (24,)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(24))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(0.01), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Summarize the model\n",
    "#model.summary()\n",
    "\n",
    "# Now fit the model\n",
    "\n",
    "model.fit([\n",
    "           [data_mda['Mon']],[data_mda['Tues']],[data_mda['Wed']],[data_mda['Thur']],[data_mda['Sat']],[data_mda['Sun']]\n",
    "          ],\n",
    "        [\n",
    "           [data_mda['Mon']],[data_mda['Tues']],[data_mda['Wed']],[data_mda['Thur']],[data_mda['Sat']],[data_mda['Sun']]\n",
    "        ]\n",
    "          ,\n",
    "          epochs=10,\n",
    "          batch_size=3,\n",
    "          validation_split=0.2,\n",
    "          verbose=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
